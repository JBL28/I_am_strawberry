{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# OpenAI API 키 설정\n",
    "openai_api_key = 'MY_OPEN_AI_KEY'\n",
    "\n",
    "# OpenAI Chat 모델 설정\n",
    "llm = ChatOpenAI(api_key=openai_api_key, model=\"gpt-4-turbo\")\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "system_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an expert in agricultural issues.\"\n",
    ")\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    \"다음은 '{title}'안에 있는 질문입니다. '{input_text}' 에 대한 딸기 농업 전문가의 답변을 생성해 주세요\"\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_template, human_template])\n",
    "\n",
    "# LLMChain 생성\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "\n",
    "# expert_data.json 파일을 업데이트하는 함수\n",
    "def update_expert_data():\n",
    "    # 기존 JSON 데이터를 불러옵니다.\n",
    "    with open('./make_eval/eval_data.json', 'r', encoding='utf-8') as f:\n",
    "        queries = json.load(f)\n",
    "    \n",
    "    # 각 항목에 대해 GPT-4 Turbo 응답을 생성하고 추가\n",
    "    for query in queries:\n",
    "        title = query['title']\n",
    "        input_text = query['input_text']\n",
    "        base_model_response = chain.run({\"title\": title, \"input_text\": input_text})\n",
    "        query['base_model_response'] = base_model_response\n",
    "\n",
    "    # 업데이트된 JSON 데이터를 파일에 다시 저장\n",
    "    with open('./make_eval/eval_data.json','w',encoding='utf-8') as f:\n",
    "        json.dump(queries, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 업데이트 함수 실행\n",
    "update_expert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "openai_api_key = 'MY_API_KEY'\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "knowledge_base_path = \"../index_store2\"\n",
    "vector_index = Chroma(persist_directory=knowledge_base_path, embedding_function=embeddings)\n",
    "retriever = vector_index.as_retriever(search_type = \"similarity\",\n",
    "                                      search_kwargw={\"k\": 3}\n",
    "                                      )\n",
    "llm = ChatOpenAI(temperature = 0,\n",
    "                 model_name=\"gpt-4-turbo\",\n",
    "                 openai_api_key = openai_api_key)\n",
    "chain = RetrievalQA.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = retriever\n",
    ")\n",
    "\n",
    "def update_expert_data():\n",
    "    # 기존 JSON 데이터를 불러옵니다.\n",
    "    with open('./make_eval/eval_data.json', 'r', encoding='utf-8') as f:\n",
    "        queries = json.load(f)\n",
    "    \n",
    "    # 각 항목에 대해 GPT-4 Turbo 응답을 생성하고 추가\n",
    "    for query in queries:\n",
    "        title = query['title']\n",
    "        input_text = query['input_text']\n",
    "        user_question = title + input_text\n",
    "        base_model_response = chain.run({\"query\": user_question})\n",
    "        query['RAG_model_response'] = base_model_response\n",
    "\n",
    "    # 업데이트된 JSON 데이터를 파일에 다시 저장\n",
    "    with open('./make_eval/eval_data.json','w',encoding='utf-8') as f:\n",
    "        json.dump(queries, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 업데이트 함수 실행\n",
    "update_expert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai_api_key = 'MY_API_KEY'\n",
    "\n",
    "# 임베딩 설정\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "\n",
    "# Chroma 데이터베이스 초기화\n",
    "knowledge_base_path = \"./index_store\"\n",
    "if os.path.exists(knowledge_base_path):\n",
    "    try:\n",
    "        shutil.rmtree(knowledge_base_path)\n",
    "        print(f\"{knowledge_base_path} 디렉토리가 성공적으로 삭제되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"{knowledge_base_path} 디렉토리 삭제에 실패했습니다. 이유: {e}\")\n",
    "\n",
    "os.makedirs(knowledge_base_path, exist_ok=True)\n",
    "\n",
    "vector_index = Chroma(embedding_function=embeddings, persist_directory=knowledge_base_path)\n",
    "\n",
    "# 폴더 경로 설정\n",
    "folder_path = \"../pdf\"\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    content = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            content += page.extract_text()\n",
    "    return content\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def process_files(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if file.endswith('.pdf'):\n",
    "                content = read_pdf(file_path)\n",
    "                vector_index.add_texts([content], [{\"source\": file_path}])\n",
    "            elif file.endswith('.json'):\n",
    "                content = read_json(file_path)\n",
    "                vector_index.add_texts([json.dumps(content)], [{\"source\": file_path}])\n",
    "\n",
    "process_files(folder_path)\n",
    "\n",
    "# 데이터베이스 저장\n",
    "vector_index.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
