{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from st_multimodal_chatinput import multimodal_chatinput\n",
    "import openai\n",
    "from bokeh.models.widgets import Button\n",
    "from bokeh.models import CustomJS\n",
    "from streamlit_bokeh_events import streamlit_bokeh_events\n",
    "from gtts import gTTS\n",
    "from langchain.chains import history_aware_retriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from getpass import getpass\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import os \n",
    "import io\n",
    "import base64\n",
    "import sys \n",
    "\n",
    "OPEN_AI_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "LANGCHAIN_API_KEY = \"YOUR_LANGCHAIN_API_KEY\"\n",
    "LANGCHAIN_PROJECT = \"YOUR_PROJECTNAME\"\n",
    "LANGCHAIN_TRACING_V2 = \"true\"\n",
    "#os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_KEY\n",
    "# vector DB path\n",
    "vector_path = \"./capstone/capstone-design/ì‚°í•™ìº¡ìŠ¤í†¤/index_store\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "vector_index = Chroma(persist_directory = vector_path,embedding_function = embeddings)\n",
    "\n",
    "retriever = vector_index.as_retriever(search_type = \"similarity\",\n",
    "                                      search_kwargs={ \"k\": 3 }\n",
    "                                      )\n",
    "\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"ë†ì—…ìš© ì½”íŒŒì¼ëŸ¿\",\n",
    "    #page_icon=\"ğŸ§Š\",\n",
    "    initial_sidebar_state=\"expanded\",\n",
    ")\n",
    "\n",
    "client = openai\n",
    "client.api_key = st.secrets['openai_api_key']\n",
    "\n",
    "#ì‘ë‹µ ìš”ì²­ í•¨ìˆ˜\n",
    "def get_completion(prompt):\n",
    "    messages = []\n",
    "    for i in st.session_state['chat']:\n",
    "        messages.append({'content':i.msg, 'role':i.sender})    \n",
    "    \n",
    "    user_input = prompt\n",
    "    input_data = {\"history\": messages, 'query':user_input}    \n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=model,\n",
    "    #     messages=messages,\n",
    "    #     temperature=temperature,\n",
    "    # )\n",
    "    \n",
    "    ##### response = qa_chain.run(input_data)\n",
    "\n",
    "    return input_data ## response\n",
    "\n",
    "#tts ìš”ì²­í•¨ìˆ˜\n",
    "def text_speech(text):\n",
    "    tts = gTTS(text=text, lang='ko')\n",
    "\n",
    "    # Save speech to a BytesIO object\n",
    "    speech_bytes = io.BytesIO()\n",
    "    tts.write_to_fp(speech_bytes)\n",
    "    speech_bytes.seek(0)\n",
    "\n",
    "    # Convert speech to base64 encoding\n",
    "    b64 = base64.b64encode(speech_bytes.read()).decode('utf-8')\n",
    "    md = f\"\"\"\n",
    "            <audio id=\"audioTag\" controls autoplay>\n",
    "            <source src=\"data:audio/mp3;base64,{b64}\"  type=\"audio/mpeg\" format=\"audio/mpeg\">\n",
    "            </audio>\n",
    "            \"\"\"\n",
    "    st.markdown(\n",
    "        md,\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "    \n",
    "#side bar\n",
    "sidebar = st.sidebar\n",
    "\n",
    "sidebar.header(\"Chatbot\")\n",
    "sidebar.text(\"copilot\")\n",
    "\n",
    "#st.session_state['api_key'] = True\n",
    "if not st.secrets['openai_api_key']:\n",
    "    sidebar.error(\":x: API ì¸ì¦ ì•ˆë¨\")\n",
    "else :\n",
    "    sidebar.success(\":white_check_mark: API ì¸ì¦ ì™„ë£Œ\")\n",
    "\n",
    "sidebar.subheader(\"Models and parameters\")\n",
    "\n",
    "model = sidebar.selectbox(\n",
    "    label=\"ëª¨ë¸ ì„ íƒ\",\n",
    "    options=[\"gpt-3.5-turbo\", \"gpt-4-turbo\", \"ëª¨ë¸3\"]\n",
    ")\n",
    "                    \n",
    "\n",
    "params = sidebar.expander(\"Parameters\")\n",
    "\n",
    "#temperature\n",
    "temperature = params.slider(\n",
    "    label=\"temperature\",\n",
    "    min_value=0.01,\n",
    "    max_value=5.00,\n",
    "    step=0.01\n",
    ")\n",
    "\n",
    "#top_p\n",
    "top_p = params.slider(\n",
    "    label=\"top_p\",\n",
    "    min_value=0.01,\n",
    "    max_value=1.00,\n",
    "    step=0.01,\n",
    "    value=0.90\n",
    ")\n",
    "\n",
    "#max_length\n",
    "max_length = params.slider(\n",
    "    label= \"max_length\",\n",
    "    min_value=32,\n",
    "    max_value=128,\n",
    "    step = 1,\n",
    "    value=120\n",
    ")\n",
    "\n",
    "# sidebar.button(\n",
    "#     label= \"Clear Chat History\"\n",
    "# )  \n",
    "\n",
    "# model setting\n",
    "\n",
    "llm = ChatOpenAI(temperature=temperature,\n",
    "                 model_name = model,\n",
    "                 openai_api_key = st.secrets['openai_api_key']\n",
    "                 )\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "weather_chain = model.bind_tools\n",
    "\n",
    "#chat\n",
    "class chat:\n",
    "    img = None\n",
    "    msg: str = None\n",
    "    sender: str = None\n",
    "    isTTS = None\n",
    "    def __init__(self, img = None, msg = None, sender = None):\n",
    "         self.msg = msg\n",
    "         self.sender = sender\n",
    "         self.img = img\n",
    "        \n",
    "\n",
    "if 'chat' not in st.session_state:\n",
    "    st.session_state['chat'] = []\n",
    "    st.session_state['chat'].append(chat(msg = \"ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\", sender='assistant')) ##ì²« ì±„íŒ…\n",
    "        \n",
    "chatContainer = st.container(height=450)\n",
    "userInput = multimodal_chatinput()\n",
    "\n",
    "for i in st.session_state['chat']:\n",
    "    with chatContainer:\n",
    "        with st.chat_message(i.sender):\n",
    "            if i.img:\n",
    "                st.image(i.img)\n",
    "            st.write(i.msg)\n",
    "\n",
    "if \"userinput_check\" not in st.session_state: #ì´ì „ì— ì¼ëŠ”ì§€ ì²´í¬\n",
    "    st.session_state['userinput_check'] = None\n",
    "\n",
    "if userInput and userInput['text'] != st.session_state['userinput_check']:\n",
    "    #ìœ ì € ì…ë ¥\n",
    "    chatting = chat()\n",
    "    if userInput['images']:\n",
    "        chatting.img = userInput['images']\n",
    "    chatting.msg = userInput['text']\n",
    "    chatting.sender = 'user'\n",
    "    st.session_state['chat'].append(chatting)\n",
    "    st.session_state['userinput_check'] = userInput['text']\n",
    "    #ë©”ì‹œì§€ ì¶œë ¥\n",
    "    with chatContainer:\n",
    "        with st.chat_message('user'):\n",
    "            if userInput['images']:\n",
    "                st.image(userInput['images'])\n",
    "            st.write(userInput['text'])\n",
    "        # for i in st.session_state['chat']:\n",
    "        #     with st.chat_message(i.sender):\n",
    "        #         if i.img:\n",
    "        #             st.image(i.img)\n",
    "        #         st.write(i.msg)\n",
    "    #ì±—ë´‡\n",
    "\n",
    "    generation = get_completion(userInput['text'])\n",
    "    response_message = generation#['generation_result']\n",
    "    #source_doc = generation['source_doc']\n",
    "\n",
    "    response = chat()\n",
    "\n",
    "    response.msg = response_message\n",
    "    response.sender = 'assistant'\n",
    "    st.session_state['chat'].append(response)\n",
    "    #ë©”ì‹œì§€ ì¶œë ¥\n",
    "    with chatContainer:\n",
    "        with st.chat_message('assistant'):\n",
    "            st.write(response_message)\n",
    "    # with chatContainer:\n",
    "    #     for i in st.session_state['chat']:\n",
    "    #         if i.sender is 'ai':\n",
    "    #             with st.chat_message(i.sender):\n",
    "    #                st.write(i.msg)\n",
    "    userInput = None\n",
    "\n",
    "\n",
    "# stt ì‚¬ìš©\n",
    "if \"tts_check\" not in st.session_state: #ì´ì „ì— ì¼ëŠ”ì§€ ì²´í¬\n",
    "    st.session_state['tts_check'] = None\n",
    "\n",
    "\n",
    "stt_button = Button(label=\"ë§í•˜ê¸°\", width=100, button_type=\"success\")\n",
    "stt_button.js_on_event(\"button_click\", CustomJS(code=\"\"\"\n",
    "    var recognition = new webkitSpeechRecognition();\n",
    "    recognition.continuous = true;\n",
    "    recognition.interimResults = true;\n",
    "\n",
    "    recognition.onresult = function (e) {\n",
    "        var value = \"\";\n",
    "        for (var i = e.resultIndex; i < e.results.length; ++i) {\n",
    "            if (e.results[i].isFinal) {\n",
    "                value += e.results[i][0].transcript;\n",
    "            }\n",
    "        }\n",
    "        if ( value != \"\") {\n",
    "            document.dispatchEvent(new CustomEvent(\"GET_TEXT\", {detail: value}));\n",
    "        }\n",
    "    }\n",
    "    recognition.start();\n",
    "    \"\"\"))\n",
    "\n",
    "\n",
    "with sidebar:\n",
    "    result = streamlit_bokeh_events(\n",
    "        stt_button,\n",
    "        events=\"GET_TEXT\",\n",
    "        key=\"listen\",\n",
    "        refresh_on_update=False,\n",
    "        override_height=40,\n",
    "        debounce_time=0,)\n",
    "        \n",
    "\n",
    "if result :\n",
    "    if \"GET_TEXT\" in result and result.get(\"GET_TEXT\") != st.session_state['tts_check']:\n",
    "        speech = chat()\n",
    "        #if result.get(\"GET_TEXT\") != st.session_state['chat'][-1].msg:\n",
    "        speech.msg = result.get(\"GET_TEXT\")\n",
    "        speech.sender = 'user'\n",
    "        st.session_state['chat'].append(speech)\n",
    "        #ìœ ì € ë©”ì‹œì§€ ì¶œë ¥\n",
    "        with chatContainer:\n",
    "            with st.chat_message('user'):\n",
    "                st.write(result.get(\"GET_TEXT\"))\n",
    "        st.session_state['tts_check'] = result.get(\"GET_TEXT\")\n",
    "\n",
    "        #ì±—ë´‡\n",
    "        generation = get_completion(result.get(\"GET_TEXT\"))\n",
    "\n",
    "        response_message = generation#['generation_result']\n",
    "\n",
    "        response = chat()\n",
    "        response.msg = response_message\n",
    "        response.sender = 'assistant'\n",
    "        response.isTTS = True\n",
    "        st.session_state['chat'].append(response)\n",
    "        \n",
    "        #ì±—ë´‡ ë©”ì‹œì§€ ì¶œë ¥\n",
    "        with chatContainer:\n",
    "            with st.chat_message('assistant'):\n",
    "                st.write(response_message)\n",
    "                text_speech(response_message)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain import Prompt, Model, Chain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from weather import (get_weather_forecast)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ì˜ˆì œ í•¨ìˆ˜ ì •ì˜\n",
    "location = location_info\n",
    "#get_weather_forecast(location)\n",
    "\n",
    "# Chromaì™€ retriever ì„¤ì •\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(embedding_function)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# ì¼ë°˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def retrieve_general_information(query):\n",
    "    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰\n",
    "    results = retriever.retrieve(query)\n",
    "    return f\"Retrieving information for query: {query}, Results: {results}\"\n",
    "\n",
    "# ë„êµ¬ ëª©ë¡ ì •ì˜\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_weather_forecast\",\n",
    "        \"description\": \"Get weather information up to 5 hours in the future using a weather API in a given location. Also this can get current weather information.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city or dong, e.g. Seoul, Gangnam\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"retrieval\",\n",
    "        \"description\": \"Use a retriever to get information from a vectorstore.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The query to retrieve information for.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ ê°ì²´ ìƒì„±\n",
    "prompt = \"Tell me about the history of Seoul.\"\n",
    "llm = ChatOpenAI(temperature=temperature,\n",
    "                 model_name = model,\n",
    "                 openai_api_key = st.secrets['openai_api_key']\n",
    "                 )\n",
    "\n",
    "# ëª¨ë¸ì˜ ì¶œë ¥ì—ì„œ í•¨ìˆ˜ í˜¸ì¶œì„ ê²°ì •í•˜ê³  ì ì ˆí•œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ì²´ì¸ ì •ì˜\n",
    "def tool_chain(prompt_text):\n",
    "    output = llm.run(prompt_text)  # ëª¨ë¸ ì‹¤í–‰\n",
    "    \n",
    "    # ì¶”ê°€ kwargsì—ì„œ í•¨ìˆ˜ í˜¸ì¶œ ì •ë³´ í™•ì¸\n",
    "    if output.additional_kwargs.get(\"tool_calls\"):\n",
    "        available_functions = {\n",
    "            \"get_weather_forecast\": get_weather_forecast,\n",
    "            \"retrieval\": retrieve_general_information\n",
    "        }\n",
    "        \n",
    "        function_name = output.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(output.additional_kwargs[\"tool_calls\"][0][\"function\"][\"arguments\"])\n",
    "        \n",
    "        if function_name == \"get_weather_forecast\":\n",
    "            function_response = function_to_call(location=function_args.get(\"location\"))\n",
    "        else:\n",
    "            function_response = function_to_call(query=function_args.get(\"query\"))\n",
    "        \n",
    "        prompt = f\"Function response: {function_response}\"\n",
    "        \n",
    "        function_chain = prompt | model.with_retry() | StrOutputParser()\n",
    "        output_with_function = function_chain.invoke({})\n",
    "        \n",
    "        # function ì ìš© í›„ output\n",
    "        print(output_with_function)\n",
    "    else:\n",
    "        # functionsë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•ŒëŠ” retrieval ì‚¬ìš©\n",
    "        result = retrieve_general_information(prompt_text)\n",
    "        return result\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "result = tool_chain(prompt)\n",
    "print(result)  # ì˜ˆì œ ì¶œë ¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
